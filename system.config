### use # to comment out the configure item

################ Status ################
mode=interactive_predict
# string: train/interactive_predict/test/save_pb_model

################ Datasets(Input/Output) ################
# 此处展示的是demo数据集所在的文件夹，训练自己数据前请自己设置一个文件夹
datasets_fold=data
train_file=ccks19+data.csv
dev_file=test.csv
test_file=test.csv

# 设置词表存放的文件夹
vocabs_dir=data/Ours/vocab/CCKS2019+/Bilstm-IDCNN-crf
# 设置训练日志存放的文件夹
log_dir=data/Ours/log/CCKS2019+/Bilstm-IDCNN-crf

delimiter=b
# string: (t: "\t";"table")|(b: "backspace";" ")|(other, e.g., '|||', ...)

# 在此处设置模型保存位置，代码支持在原始模型上继续训练，新数据或从头训练一定要改！
checkpoints_dir=data/Ours/checkpoints/CCKS2019+/Bilstm-IDCNN-crf

# 模型的名字
checkpoint_name=model

################ Labeling Scheme ################
label_scheme=BIO
# string: BIO/BIESO

label_level=2
# int, 1:BIO/BIESO; 2:BIO/BIESO + suffix
# max to 2

hyphen=-
# string: -|_, for connecting the prefix and suffix: `B_PER', `I_LOC'

#'''suffix=[Symptom,Drug,Drug_Category,Medical_Examination,Operation]
#'''suffix=[AMO,ANT,DIS,DRU,DUR,FRE,LEV,MET,OPE,REA,SID,SYM,TES,TRE,TSV]
suffix=[Dsa,Chk,Ins,Sur,Med,Ana]
# unnecessary if label_level=1


measuring_metrics=[precision,recall,f1,accuracy]
# string: accuracy|precision|recall|f1
# f1 is compulsory

################ Model Configuration ################

use_pretrained_model=False
pretrained_model=Bert
# Bert/ALBert/Roberta/XLNet/DistillBert/Electra/MiniLM
finetune=False

use_middle_model=True
middle_model=bilstm+idcnn
# bilstm/idcnn/bilstm+idcnn

# 不使用预训练模型的时候词表的维度
embedding_dim=128
# int

# 选择lstm时，隐藏层大小
hidden_dim=128

#交叉验证
kf_split=False
# kf_split = 10

# 选择idcnn时filter的个数
filter_nums=64

idcnn_nums=3
idcnn_numes=6

max_sequence_length=256
# int, cautions! set as a LARGE number as possible,
# this will be kept during training and inferring, text having length larger than this will be truncated.

CUDA_VISIBLE_DEVICES=0
# int, -1:CPU, [0,]:GPU
# coincides with tf.CUDA_VISIBLE_DEVICES

seed=23

################ Training Settings ################
epoch=100
batch_size=32

dropout=0.3
# 微调预训练模型时，建议设置为5e-5
learning_rate=1e-3

optimizer=Adam
# string: SGD/Adagrad/AdaDelta/RMSprop/Adam/AdamW

use_gan=False
gan_method=pgd
# fgm/pgd

checkpoints_max_to_keep=3
print_per_batch=10

is_early_stop=True
patient=5
# unnecessary if is_early_stop=False

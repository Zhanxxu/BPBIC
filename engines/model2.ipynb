{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.text.crf import crf_log_likelihood\n",
    "from engines.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class NerModel(tf.keras.Model, ABC):\n",
    "    def __init__(self, configs, vocab_size,num_classes):\n",
    "        super(NerModel, self).__init__()\n",
    "        self.use_pretrained_model = configs.use_pretrained_model\n",
    "        self.finetune = configs.finetune\n",
    "\n",
    "        if self.use_pretrained_model and self.finetune:\n",
    "            if configs.pretrained_model == 'Bert':\n",
    "                from transformers import TFBertModel\n",
    "                self.pretrained_model = TFBertModel.from_pretrained('bert-base-chinese')\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, configs.embedding_dim, mask_zero=True)\n",
    "\n",
    "        self.use_middle_model = configs.use_middle_model\n",
    "        self.middle_model = configs.middle_model\n",
    "        if self.use_middle_model:\n",
    "            if self.middle_model == 'bilstm':\n",
    "                self.hidden_dim = configs.hidden_dim\n",
    "                self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True))\n",
    "                self.attention = Attention()\n",
    "            if self.middle_model == 'idcnn':\n",
    "                filter_nums = configs.filter_nums\n",
    "                self.idcnn_nums = configs.idcnn_nums\n",
    "                self.cnn = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Conv1D(filters=filter_nums, kernel_size=3, activation='relu',\n",
    "                                           padding='same', dilation_rate=1),\n",
    "                    tf.keras.layers.Conv1D(filters=filter_nums, kernel_size=3, activation='relu',\n",
    "                                           padding='same', dilation_rate=1),\n",
    "                    tf.keras.layers.Conv1D(filters=filter_nums, kernel_size=3, activation='relu',\n",
    "                                           padding='same', dilation_rate=2)])\n",
    "                self.idcnn = [self.cnn for _ in range(self.idcnn_nums)]\n",
    "            if self.middle_model == 'bilstm+idcnn':\n",
    "                self.hidden_dim = configs.hidden_dim\n",
    "                self.bilstm = tf.keras.layers.Bidirectional(\n",
    "                    tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True))\n",
    "                filter_nums = configs.filter_nums\n",
    "                self.idcnn_nums = configs.idcnn_nums\n",
    "                self.cnn = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Conv1D(filters=filter_nums, kernel_size=3, activation='relu',\n",
    "                                           padding='same', dilation_rate=1),\n",
    "                    tf.keras.layers.Conv1D(filters=filter_nums, kernel_size=3, activation='relu',\n",
    "                                           padding='same', dilation_rate=1),\n",
    "                    tf.keras.layers.Conv1D(filters=filter_nums, kernel_size=3, activation='relu',\n",
    "                                           padding='same', dilation_rate=2)])\n",
    "                self.idcnn = [self.cnn for _ in range(self.idcnn_nums)]\n",
    "                #self.hidden_dim = configs.hidden_dim\n",
    "                #self.bilstm = tf.keras.layers.Bidirectional(\n",
    "                 #   tf.keras.layers.LSTM(self.hidden_dim, return_sequences=True))\n",
    "\n",
    "\n",
    "        self.dropout_rate = configs.dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.dense = tf.keras.layers.Dense(num_classes)\n",
    "        self.transition_params = tf.Variable(tf.random.uniform(shape=(num_classes, num_classes)))\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, inputs_length, targets, training=None):\n",
    "        if self.use_pretrained_model:\n",
    "            if self.finetune:\n",
    "                embedding_inputs = self.pretrained_model(inputs[0], attention_mask=inputs[1])[0]\n",
    "            else:\n",
    "                embedding_inputs = inputs\n",
    "        else:\n",
    "            embedding_inputs = self.embedding(inputs)\n",
    "\n",
    "        outputs = self.dropout(embedding_inputs, training)\n",
    "\n",
    "        if self.use_middle_model:\n",
    "            if self.middle_model == 'bilstm':\n",
    "                outputs = self.bilstm(outputs)\n",
    "                #outputs = self.attention(bioutputs)\n",
    "            if self.middle_model == 'idcnn':\n",
    "                cnn_outputs = [idcnn(outputs) for idcnn in self.idcnn]\n",
    "                if self.idcnn_nums == 1:\n",
    "                    outputs = cnn_outputs[0]\n",
    "                else:\n",
    "                    outputs = tf.keras.layers.concatenate(cnn_outputs, axis=-1, name='concatenate')\n",
    "            if self.middle_model == 'bilstm+idcnn':\n",
    "\n",
    "                outputs = self.bilstm(outputs)\n",
    "                cnn_outputs = [idcnn(outputs) for idcnn in self.idcnn]\n",
    "                if self.idcnn_nums == 1:\n",
    "                    outputs = cnn_outputs[0]\n",
    "                else:\n",
    "                    outputs = tf.keras.layers.concatenate(cnn_outputs, axis=-1, name='concatenate')\n",
    "                #outputs = self.bilstm(outputs)\n",
    "                #bdoutputs = tf.keras.layers.concatenate([bioutputs,outputs],axis=-1,name='concatenate')\n",
    "                #outputs = tf.keras.layers.concatenate([bdoutputs, bioutputs], axis=-1, name='concatenate')\n",
    "\n",
    "\n",
    "        logits = self.dense(outputs)\n",
    "        tensor_targets = tf.convert_to_tensor(targets, dtype=tf.int32)\n",
    "        log_likelihood, self.transition_params = crf_log_likelihood(\n",
    "            logits, tensor_targets, inputs_length, transition_params=self.transition_params)\n",
    "        return logits, log_likelihood, self.transition_params\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}